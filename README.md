# Adversarial Robustness in Deep Learning for Image Classification

A comprehensive implementation of adversarial robustness techniques for deep learning image classification models. This project demonstrates how to train robust models, generate adversarial examples, and evaluate model robustness against various attacks.

## ğŸ¯ Features

- **Multiple Attack Methods**: FGSM and PGD adversarial attacks
- **Adversarial Training**: Defense mechanism to improve model robustness
- **Comprehensive Evaluation**: Test models against various attack strengths
- **Visualization Demo**: See adversarial examples and their effects
- **Multiple Datasets**: Support for MNIST, CIFAR-10, and CIFAR-100

## ğŸ“ Project Structure

```
adversarial-robustness/
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ cnn.py                 # CNN model architecture
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ data_loader.py         # Data loading utilities
â”‚   â””â”€â”€ train.py               # Standard training script
â”‚
â”œâ”€â”€ attacks/
â”‚   â”œâ”€â”€ fgsm.py               # Fast Gradient Sign Method attack
â”‚   â””â”€â”€ pgd.py                # Projected Gradient Descent attack
â”‚
â”œâ”€â”€ defense/
â”‚   â””â”€â”€ adv_training.py       # Adversarial training defense
â”‚
â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ robustness_eval.py    # Robustness evaluation metrics
â”‚
â”œâ”€â”€ demo/
â”‚   â””â”€â”€ app.py                # Demo and visualization application
â”‚
â”œâ”€â”€ checkpoints/              # Model checkpoints directory
â”œâ”€â”€ main.py                   # Main entry point
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md                 # This file
```

## ğŸš€ Quick Start

### Installation

1. Clone the repository:
```bash
git clone https://github.com/Aashi-ghub/Adversial-robustness-Deep-learning-with-image-classification.git
cd Adversial-robustness-Deep-learning-with-image-classification
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

### Usage

#### 1. Standard Training

Train a model using standard (clean) training:

```bash
python main.py --mode train --dataset cifar10 --epochs 10 --lr 0.01
```

#### 2. Adversarial Training

Train a robust model using adversarial training:

```bash
python main.py --mode adv-train --dataset cifar10 --epochs 10 --attack pgd --epsilon 0.3
```

#### 3. Evaluate Model Robustness

Evaluate a trained model against adversarial attacks:

```bash
python main.py --mode evaluate --load-path checkpoints/model.pth --dataset cifar10
```

#### 4. Run Demo

Generate and visualize adversarial examples:

```bash
python main.py --mode demo --load-path checkpoints/model.pth --dataset cifar10
```

## ğŸ“Š Command Line Arguments

| Argument | Description | Default | Choices |
|----------|-------------|---------|---------|
| `--mode` | Operating mode | `train` | `train`, `adv-train`, `evaluate`, `demo` |
| `--dataset` | Dataset to use | `cifar10` | `mnist`, `cifar10`, `cifar100` |
| `--epochs` | Number of training epochs | `10` | - |
| `--batch-size` | Batch size for training | `128` | - |
| `--lr` | Learning rate | `0.01` | - |
| `--attack` | Attack method for adversarial training | `pgd` | `fgsm`, `pgd` |
| `--epsilon` | Perturbation magnitude | `0.3` | - |
| `--save-path` | Path to save model | `checkpoints/model.pth` | - |
| `--load-path` | Path to load pretrained model | `None` | - |
| `--device` | Device to use | `cuda` | `cuda`, `cpu` |

## ğŸ”¬ Attack Methods

### FGSM (Fast Gradient Sign Method)
A simple one-step attack that perturbs the input in the direction of the gradient:
```
x_adv = x + Îµ * sign(âˆ‡_x L(Î¸, x, y))
```

### PGD (Projected Gradient Descent)
An iterative version of FGSM that takes multiple small steps:
```
x_adv^(t+1) = Î _{x+S} (x_adv^(t) + Î± * sign(âˆ‡_x L(Î¸, x_adv^(t), y)))
```

## ğŸ›¡ï¸ Defense: Adversarial Training

Adversarial training improves model robustness by training on both clean and adversarial examples:
```
min_Î¸ E_{(x,y)~D} [max_{||Î´||â‰¤Îµ} L(Î¸, x+Î´, y)]
```

## ğŸ“ˆ Evaluation Metrics

The evaluation module provides:
- Clean accuracy (performance on original images)
- Adversarial accuracy (performance under attack)
- Robustness curves across different perturbation magnitudes
- Comparison between different attack methods

## ğŸ¨ Visualization

The demo application generates visualizations showing:
- Original images
- Perturbations added by attacks
- Resulting adversarial examples
- Differences between original and adversarial images
- Model predictions on clean vs adversarial inputs

## ğŸ“š Example Workflow

Complete workflow for training and evaluating a robust model:

```bash
# 1. Train a standard model
python main.py --mode train --dataset cifar10 --epochs 20 --save-path checkpoints/clean_model.pth

# 2. Evaluate standard model's robustness
python main.py --mode evaluate --load-path checkpoints/clean_model.pth --dataset cifar10

# 3. Train an adversarially robust model
python main.py --mode adv-train --dataset cifar10 --epochs 20 --attack pgd --epsilon 0.3 --save-path checkpoints/robust_model.pth

# 4. Evaluate robust model
python main.py --mode evaluate --load-path checkpoints/robust_model.pth --dataset cifar10

# 5. Generate visualizations
python main.py --mode demo --load-path checkpoints/robust_model.pth --dataset cifar10
```

## ğŸ”¬ Research Background

### Adversarial Examples
Adversarial examples are inputs intentionally designed to cause machine learning models to make mistakes. Even small, imperceptible perturbations can fool state-of-the-art deep learning models.

### Why This Matters
- **Security**: Adversarial attacks pose security risks in real-world applications
- **Robustness**: Understanding vulnerabilities helps build more reliable models
- **Interpretability**: Studying adversarial examples reveals how models make decisions

## ğŸ“– References

- Goodfellow et al., "Explaining and Harnessing Adversarial Examples" (FGSM)
- Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks" (PGD)
- Adversarial training and robustness evaluation methodologies

## ğŸ¤ Contributing

Contributions are welcome! Feel free to:
- Report bugs
- Suggest new features
- Add new attack or defense methods
- Improve documentation

## ğŸ“ License

This project is available for educational and research purposes.

## ğŸ‘¤ Author

Created by [Aashi-ghub](https://github.com/Aashi-ghub)

## ğŸ™ Acknowledgments

This project implements techniques from adversarial robustness research and is intended for educational purposes to understand and improve deep learning security.